{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "sys.path.append('./')\n",
    "\n",
    "from tqdm import tqdm as tqdm_base\n",
    "def tqdm(*args, **kwargs):\n",
    "    if hasattr(tqdm_base, '_instances'):\n",
    "        for instance in list(tqdm_base._instances):\n",
    "            tqdm_base._decr_instances(instance)\n",
    "    return tqdm_base(*args, **kwargs)\n",
    "\n",
    "from ncsnv2.models        import get_sigmas\n",
    "from ncsnv2.models.ema    import EMAHelper\n",
    "from ncsnv2.models.ncsnv2 import NCSNv2Deepest\n",
    "from ncsnv2.losses        import get_optimizer\n",
    "from ncsnv2.losses.dsm    import anneal_dsm_score_estimation\n",
    "\n",
    "import scipy.io as sio\n",
    "import random\n",
    "from data.loaders          import Channels\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dotmap import DotMap\n",
    "\n",
    "from data.sample_generator import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always !!!\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32       = False\n",
    "\n",
    "# GPU\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]    = \"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\";\n",
    "\n",
    "# Model config\n",
    "config          = DotMap()\n",
    "config.device   = 'cuda:0'\n",
    "# Inner model\n",
    "config.model.ema           = True\n",
    "config.model.ema_rate      = 0.999\n",
    "config.model.normalization = 'InstanceNorm++'\n",
    "config.model.nonlinearity  = 'elu'\n",
    "config.model.sigma_dist    = 'geometric'\n",
    "config.model.num_classes   = 2311 # Number of train sigmas and 'N'\n",
    "config.model.ngf           = 32\n",
    "\n",
    "# Optimizer\n",
    "config.optim.weight_decay  = 0.000 # No weight decay\n",
    "config.optim.optimizer     = 'Adam'\n",
    "config.optim.lr            = 0.0001\n",
    "config.optim.beta1         = 0.9\n",
    "config.optim.amsgrad       = False\n",
    "config.optim.eps           = 0.001\n",
    "\n",
    "# Training\n",
    "config.training.batch_size     = 32\n",
    "config.training.num_workers    = 4\n",
    "config.training.n_epochs       = 40\n",
    "config.training.anneal_power   = 2\n",
    "config.training.log_all_sigmas = False\n",
    "config.training.eval_freq      = 50 # In epochs\n",
    "\n",
    "# Data\n",
    "config.data.channel        = '3GPP' # Training and validation\n",
    "config.data.channels       = 2 # {Re, Im}\n",
    "config.data.num_pilots     = 8\n",
    "config.data.noise_std      = 0.01 # 'Beta' in paper\n",
    "config.data.image_size     = [64, 32] # Channel size = Nr x Nt\n",
    "config.data.mixed_channels = False\n",
    "config.data.norm_channels  = 'global' # Optional, no major impact\n",
    "\n",
    "# Universal seeds\n",
    "train_seed, val_seed = 1234, 4321\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets and loaders for channels\n",
    "train_samples = 7500\n",
    "mat_contents = sio.loadmat('data/H_bank_64.mat')\n",
    "H = mat_contents['H_bank']\n",
    "H_complex = torch.tensor(H[:train_samples, :, :]).detach().numpy()#Pick up NT random users from 100.\n",
    "\n",
    "\n",
    "dataset     = Channels(train_seed, config, H = H_complex, norm=config.data.norm_channels)\n",
    "dataloader  = DataLoader(dataset, batch_size=config.training.batch_size, \n",
    "                         shuffle=True, num_workers=config.training.num_workers,\n",
    "                         drop_last=True)\n",
    "\n",
    "\n",
    "# Validation data\n",
    "H_val_complex = torch.tensor(H[train_samples:9500, :, :]).detach().numpy()#Pick up NT random users from 100.\n",
    "val_samples = H_val_complex.shape[0]\n",
    "\n",
    "val_config = copy.deepcopy(config)\n",
    "val_datasets = Channels(val_seed, val_config, H = H_val_complex,  norm=config.data.norm_channels)\n",
    "val_loaders = DataLoader(val_datasets, \n",
    "                         batch_size=len(val_datasets),\n",
    "                         shuffle=False, \n",
    "                         num_workers=0, \n",
    "                         drop_last=True)\n",
    "val_iters = iter(val_loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-determined values\n",
    "config.model.sigma_begin = 30 # !!! For CDL-C\n",
    "config.model.sigma_rate = 0.995 # !!! For everything\n",
    "config.model.sigma_end  = config.model.sigma_begin * config.model.sigma_rate ** (config.model.num_classes - 1)\n",
    "\n",
    "# Choose the step size (epsilon) according to [Song '20]\n",
    "candidate_steps = np.logspace(-13, -8, 1000)\n",
    "step_criterion  = np.zeros((len(candidate_steps)))\n",
    "gamma_rate      = 1 / config.model.sigma_rate\n",
    "for idx, step in enumerate(candidate_steps):\n",
    "    step_criterion[idx] = (1 - step / config.model.sigma_end ** 2) \\\n",
    "        ** (2 * config.model.num_classes) * (gamma_rate ** 2 -\n",
    "            2 * step / (config.model.sigma_end ** 2 - config.model.sigma_end ** 2 * (\n",
    "                1 - step / config.model.sigma_end ** 2) ** 2)) + \\\n",
    "            2 * step / (config.model.sigma_end ** 2 - config.model.sigma_end ** 2 * (\n",
    "                1 - step / config.model.sigma_end ** 2) ** 2)\n",
    "best_idx = np.argmin(np.abs(step_criterion - 1.))\n",
    "config.model.step_size = candidate_steps[best_idx]\n",
    "\n",
    "# Get the model\n",
    "diffuser = NCSNv2Deepest(config)\n",
    "diffuser = diffuser.cuda()\n",
    "\n",
    "# Get optimizer\n",
    "optimizer = get_optimizer(config, diffuser.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter\n",
    "start_epoch = 0\n",
    "step = 0\n",
    "\n",
    "if config.model.ema:\n",
    "    ema_helper = EMAHelper(mu=config.model.ema_rate)\n",
    "    ema_helper.register(diffuser)\n",
    "\n",
    "# Get a collection of sigma values\n",
    "sigmas = get_sigmas(config)\n",
    "\n",
    "# Always the same initial points and data for validation\n",
    "val_H_list = []\n",
    "\n",
    "val_sample = next(val_iters)\n",
    "# val_H_list.append(val_sample['H_herm'].cuda())\n",
    "\n",
    "# More logging\n",
    "config.log_path = 'models/sigmaT%.1f' % (config.model.sigma_begin)\n",
    "\n",
    "if not os.path.exists(config.log_path):\n",
    "    os.makedirs(config.log_path)\n",
    "    \n",
    "# No sigma logging\n",
    "hook = test_hook = None\n",
    "\n",
    "# Logged metrics\n",
    "train_loss, val_loss  = [], []\n",
    "val_errors, val_epoch = [], []\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, config.training.n_epochs)):\n",
    "    for i, sample in tqdm(enumerate(dataloader)):\n",
    "        # Safety check\n",
    "        diffuser.train()\n",
    "        step += 1\n",
    "        \n",
    "        # Move data to device\n",
    "        for key in sample:\n",
    "            sample[key] = sample[key].cuda()\n",
    "        \n",
    "        # Get loss on Hermitian channels\n",
    "        loss = anneal_dsm_score_estimation(\n",
    "            diffuser, sample['H_herm'], sigmas, None, \n",
    "            config.training.anneal_power, hook)\n",
    "        \n",
    "        # Keep a running loss\n",
    "        if step == 1:\n",
    "            running_loss = loss.item()\n",
    "        else:\n",
    "            running_loss = 0.99 * running_loss + 0.01 * loss.item()\n",
    "        # Log\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        # Step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # EMA update\n",
    "        if config.model.ema:\n",
    "            ema_helper.update(diffuser)\n",
    "            \n",
    "        # Verbose\n",
    "        if step % 100 == 0:\n",
    "            if config.model.ema:\n",
    "                val_score = ema_helper.ema_copy(diffuser)\n",
    "            else:\n",
    "                val_score = diffuser\n",
    "            \n",
    "            # For each validation setup\n",
    "            local_val_losses = []\n",
    "            for idx in range(len(config.data.spacing_list)):\n",
    "                with torch.no_grad():\n",
    "                    val_dsm_loss = \\\n",
    "                        anneal_dsm_score_estimation(\n",
    "                            val_score, val_H_list[idx],\n",
    "                            sigmas, None,\n",
    "                            config.training.anneal_power,\n",
    "                            hook=test_hook)\n",
    "                # Store\n",
    "                local_val_losses.append(val_dsm_loss.item())\n",
    "            # Sanity delete\n",
    "            del val_score\n",
    "            # Log\n",
    "            val_loss.append(local_val_losses)\n",
    "                \n",
    "            # Print\n",
    "            if len(local_val_losses) == 1:\n",
    "                print('Epoch %d, Step %d, Train Loss (EMA) %.3f, Val. Loss %.3f' % (epoch, step, running_loss, \n",
    "                                                                                    local_val_losses[0]))\n",
    "            elif len(local_val_losses) == 2:\n",
    "                print('Epoch %d, Step %d, Train Loss (EMA) %.3f, Val. Loss (Split) %.3f %.3f' % (epoch, step, running_loss, \n",
    "                                                                                                local_val_losses[0], local_val_losses[1]))\n",
    "        \n",
    "# Save snapshot\n",
    "torch.save({'model_state': diffuser.state_dict(),\n",
    "            'optim_state': optimizer.state_dict(),\n",
    "            'config': config,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_errors': val_errors,\n",
    "            'val_epoch': val_epoch}, \n",
    "   os.path.join(config.log_path, 'final_model_3gpp_64.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac4117b4f16cf43a7b4d29a08f995b98bcec8327806e376c489c9fe767f71064"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
